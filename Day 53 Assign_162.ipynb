{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98e0d6-4542-49ce-af81-3c716e3c8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9956c-5ea7-4c45-8c7d-eb6c303aa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'Ensemble_Techniques_XGBM_Data.csv'  # Ensure the correct dataset path\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display first five rows\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values (if any)\n",
    "df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Splitting dataset into features and target\n",
    "if 'target' in df.columns:\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "else:\n",
    "    raise ValueError(\"Target column not found in dataset. Please check column names.\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree as baseline model\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "# Feature Importance from Random Forest\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feat_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(feat_importance_df.head())\n",
    "\n",
    "# Boosting Algorithms\n",
    "# AdaBoost\n",
    "adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_adaboost))\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "\n",
    "# XGBoost with Hyperparameter Tuning\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best XGBoost model\n",
    "y_pred_xgb = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"Best XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n",
    "# Model Comparison\n",
    "models = ['Decision Tree', 'Bagging', 'Random Forest', 'AdaBoost', 'Gradient Boosting', 'XGBoost']\n",
    "accuracies = [accuracy_score(y_test, y_pred_dt), accuracy_score(y_test, y_pred_bagging), accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_adaboost), accuracy_score(y_test, y_pred_gb), accuracy_score(y_test, y_pred_xgb)]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=models, y=accuracies, palette='viridis')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abd81a-4cbe-449d-be30-9361e08c014c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
